"""
è—¥ä¸¸æª¢æ¸¬æ ¸å¿ƒæ¨¡çµ„ï¼šæ•´åˆåœ–åƒé è™•ç†ã€æ¨¡å‹æ¨ç†èˆ‡çµæœæ¨™è¨»
"""
import os
import json
import logging
import base64
from io import BytesIO
from typing import List, Dict, Optional, Tuple
import numpy as np
import cv2
import requests
import onnxruntime as ort
from PIL import Image, ImageDraw, ImageFont

from .config import *

logger = logging.getLogger(__name__)

class PillDetector:
    """è—¥ä¸¸æª¢æ¸¬å™¨ä¸»é¡"""
    
    def __init__(self):
        self.onnx_session = None
        self.class_names = None
        
    async def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹å’Œé¡åˆ¥åç¨±"""
        await self._load_model()
        await self._load_class_names()
        
    async def _load_model(self):
        """è¼‰å…¥ ONNX æ¨¡å‹"""
        try:
            if not os.path.exists(MODEL_PATH):
                raise FileNotFoundError(f"ONNX æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {MODEL_PATH}")
                
            self.onnx_session = ort.InferenceSession(
                MODEL_PATH,
                providers=['CPUExecutionProvider']
            )
            logger.info("âœ… ONNX æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            logger.info(f"ğŸ” æ¨¡å‹è¼¸å…¥: {[inp.name for inp in self.onnx_session.get_inputs()]}")
            logger.info(f"ğŸ“¤ æ¨¡å‹è¼¸å‡º: {[out.name for out in self.onnx_session.get_outputs()]}")
            
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥ ONNX æ¨¡å‹å¤±æ•—: {e}")
            raise
            
    async def _load_class_names(self):
        """è¼‰å…¥é¡åˆ¥åç¨±"""
        try:
            if not os.path.exists(COCO_ANNOTATIONS_PATH):
                raise FileNotFoundError(f"COCO æ¨™è¨»æª”æ¡ˆä¸å­˜åœ¨: {COCO_ANNOTATIONS_PATH}")
                
            with open(COCO_ANNOTATIONS_PATH, "r", encoding='utf-8') as f:
                coco_data = json.load(f)
            
            categories = sorted(coco_data['categories'], key=lambda x: x['id'])
            self.class_names = [cat['name'] for cat in categories]
            
            logger.info(f"âœ… æˆåŠŸè¼‰å…¥ {len(self.class_names)} å€‹é¡åˆ¥")
            logger.info(f"ğŸ“‹ é¡åˆ¥: {self.class_names}")
            
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥é¡åˆ¥åç¨±å¤±æ•—: {e}")
            raise
            
    def preprocess_image(self, image_array: np.ndarray) -> np.ndarray:
        """åœ–åƒé è™•ç†ï¼šæµç¨‹èˆ‡ main_legacy.py å®Œå…¨ä¸€è‡´"""
        try:
            # æ­¥é©Ÿ1: è½‰æ›ç‚º CHW ä¸¦æ­£è¦åŒ–åˆ° [0,1]
            tensor_like = image_array.transpose((2, 0, 1)).astype(np.float32) / 255.0
            
            # æ­¥é©Ÿ2: ImageNet æ­£è¦åŒ–
            means = np.array(IMAGENET_MEAN, dtype=np.float32).reshape(-1, 1, 1)
            stds = np.array(IMAGENET_STD, dtype=np.float32).reshape(-1, 1, 1)
            normalized = (tensor_like - means) / stds
            
            # æ­¥é©Ÿ3: èª¿æ•´åˆ°æ¨¡å‹è¼¸å…¥å°ºå¯¸ (ä½¿ç”¨ OpenCV ä»¥åŒ¹é…åŸå§‹æµç¨‹)
            hwc_normalized = normalized.transpose((1, 2, 0))  # CHW -> HWC for OpenCV
            resized_hwc = cv2.resize(hwc_normalized, INPUT_SIZE, interpolation=cv2.INTER_LINEAR)
            resized_chw = resized_hwc.transpose((2, 0, 1))  # HWC -> CHW
            
            # æ­¥é©Ÿ4: æ·»åŠ  batch ç¶­åº¦
            batched = np.expand_dims(resized_chw, axis=0)
            
            return batched
            
        except Exception as e:
            logger.error(f"âŒ åœ–åƒé è™•ç†å¤±æ•—: {e}")
            raise
        
    def postprocess_results(self, outputs, original_size: Tuple[int, int]) -> List[Dict]:
        """æ¨¡å‹æ¨ç†çµæœå¾Œè™•ç†ï¼Œç”¢ç”Ÿæ¨™æº–åŒ–æª¢æ¸¬çµæœ"""
        try:
            # æå–é æ¸¬çµæœ - æŒ‰ç…§ main_legacy.py çš„æ–¹å¼
            pred_boxes, pred_logits = outputs[0][0], outputs[1][0]  # ç§»é™¤ batch ç¶­åº¦
            
            # Sigmoid æ¿€æ´»
            prob = 1.0 / (1.0 + np.exp(-pred_logits))
            
            # Top-K é¸æ“‡
            prob_flat = prob.reshape(-1)
            topk_indices = np.argpartition(prob_flat, -TOP_K)[-TOP_K:]
            topk_indices = topk_indices[np.argsort(prob_flat[topk_indices])[::-1]]
            
            scores = prob_flat[topk_indices]
            topk_boxes = topk_indices // pred_logits.shape[1]
            labels = topk_indices % pred_logits.shape[1]
            
            # é‚Šç•Œæ¡†æ ¼å¼è½‰æ› cxcywh -> xyxy (å‘é‡åŒ–æ“ä½œ)
            def box_cxcywh_to_xyxy(boxes):
                cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
                x1, y1 = cx - 0.5 * w, cy - 0.5 * h
                x2, y2 = cx + 0.5 * w, cy + 0.5 * h
                return np.stack([x1, y1, x2, y2], axis=1)
            
            boxes_xyxy = box_cxcywh_to_xyxy(pred_boxes)
            selected_boxes = boxes_xyxy[topk_boxes]
            
            # ç¸®æ”¾åˆ°åŸå§‹å°ºå¯¸
            img_w, img_h = original_size
            scale_fct = np.array([img_w, img_h, img_w, img_h])
            final_boxes = selected_boxes * scale_fct
            
            # æ‡‰ç”¨é–¾å€¼éæ¿¾
            valid_mask = scores >= CONFIDENCE_THRESHOLD
            if not np.any(valid_mask):
                logger.info("â„¹ï¸ æ²’æœ‰æª¢æ¸¬çµæœè¶…éé–¾å€¼")
                return []
            
            valid_boxes = final_boxes[valid_mask]
            valid_confidences = scores[valid_mask]
            valid_class_ids = labels[valid_mask]
            
            # åº§æ¨™ç¯„åœé™åˆ¶
            valid_boxes[:, [0, 2]] = np.clip(valid_boxes[:, [0, 2]], 0, img_w)
            valid_boxes[:, [1, 3]] = np.clip(valid_boxes[:, [1, 3]], 0, img_h)
            
            # è½‰æ›ç‚º API è¼¸å‡ºæ ¼å¼
            results = []
            for i in range(len(valid_boxes)):
                x1, y1, x2, y2 = valid_boxes[i]
                results.append({
                    'class_id': int(valid_class_ids[i]),
                    'class_name': self.class_names[valid_class_ids[i]] if self.class_names else f'Class_{valid_class_ids[i]}',
                    'confidence': float(valid_confidences[i]),
                    'bbox': [int(x1), int(y1), int(x2), int(y2)]
                })
                    
            return results
                
        except Exception as e:
            logger.error(f"âŒ å¾Œè™•ç†å¤±æ•—: {e}")
            return []
            
    def get_optimal_font(self, font_size: int):
        """å–å¾—æœ€ä½³å¯ç”¨å­—é«”ï¼ˆèˆ‡ main_legacy.py ç›¸åŒé‚è¼¯ï¼‰"""
        font_paths = [
            "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf",
            "/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf",
            "/usr/share/fonts/TTF/DejaVuSans-Bold.ttf",
            "/System/Library/Fonts/Helvetica.ttc",
            "C:/Windows/Fonts/arialbd.ttf",
        ]
        
        for font_path in font_paths:
            try:
                return ImageFont.truetype(font_path, font_size)
            except:
                continue
        
        return ImageFont.load_default()

    def calculate_smart_label_positions(self, detections: List[Dict], image_size: Tuple[int, int], font_size: int) -> List[Optional[Tuple[int, int]]]:
        """æ™ºèƒ½è¨ˆç®—æ¨™ç±¤ä½ç½®ï¼Œé¿å…é‡ç–Šèˆ‡é®æ“‹ï¼ˆèˆ‡ main_legacy.py ä¸€è‡´ï¼‰"""
        w, h = image_size
        
        if len(detections) == 0:
            return []
        
        label_height = font_size + 8
        max_label_width = font_size * 12
        
        label_positions = []
        occupied_regions = []
        
        # æŒ‰ä¿¡å¿ƒåº¦æ’åº
        sorted_detections = sorted(enumerate(detections), key=lambda x: x[1]['confidence'], reverse=True)
        
        for original_idx, det in sorted_detections:
            bbox = det['bbox']
            class_name = det['class_name']
            conf = det['confidence']
            
            label_text = f"{class_name} {conf:.2f}"
            label_width = min(len(label_text) * font_size * 0.6, max_label_width)
            
            x1, y1, x2, y2 = bbox
            
            # å€™é¸ä½ç½® - å¢åŠ æ›´å¤šé¸é …
            candidates = [
                # ä¸Šæ–¹ - å¤šå€‹ä½ç½®
                (x1, y1 - label_height - 5, x1 + label_width, y1 - 5),
                (x1 + (x2-x1)//4, y1 - label_height - 5, x1 + (x2-x1)//4 + label_width, y1 - 5),
                (max(0, x2 - label_width), y1 - label_height - 5, x2, y1 - 5),
                
                # ä¸‹æ–¹ - å¤šå€‹ä½ç½®  
                (x1, y2 + 5, x1 + label_width, y2 + 5 + label_height),
                (x1 + (x2-x1)//4, y2 + 5, x1 + (x2-x1)//4 + label_width, y2 + 5 + label_height),
                (max(0, x2 - label_width), y2 + 5, x2, y2 + 5 + label_height),
                
                # å·¦å´
                (x1 - label_width - 5, y1, x1 - 5, y1 + label_height),
                (x1 - label_width - 5, y1 + (y2-y1)//4, x1 - 5, y1 + (y2-y1)//4 + label_height),
                
                # å³å´
                (x2 + 5, y1, x2 + 5 + label_width, y1 + label_height),
                (x2 + 5, y1 + (y2-y1)//4, x2 + 5 + label_width, y1 + (y2-y1)//4 + label_height),
                
                # æ¡†å…§ä¸Šæ–¹
                (x1 + 5, y1 + 5, x1 + 5 + label_width, y1 + 5 + label_height),
                # æ¡†å…§ä¸‹æ–¹
                (x1 + 5, y2 - label_height - 5, x1 + 5 + label_width, y2 - 5),
            ]
            
            # æ‰¾æœ€ä½³ä½ç½®
            best_position = None
            for pos_x1, pos_y1, pos_x2, pos_y2 in candidates:
                # 1. é‚Šç•Œæª¢æŸ¥
                if pos_x1 < 0 or pos_y1 < 0 or pos_x2 > w or pos_y2 > h:
                    continue
                
                # 2. èˆ‡å…¶ä»–æ¨™ç±¤é‡ç–Šæª¢æŸ¥
                overlaps_label = False
                for occupied in occupied_regions:
                    if not (pos_x2 < occupied[0] or pos_x1 > occupied[2] or 
                           pos_y2 < occupied[1] or pos_y1 > occupied[3]):
                        overlaps_label = True
                        break
                
                if overlaps_label:
                    continue
                
                # 3. æª¢æŸ¥æ˜¯å¦é®æ“‹å…¶ä»–æª¢æ¸¬æ¡†
                blocks_other_boxes = False
                for other_det in detections:
                    if other_det == det:  # è·³éè‡ªå·±
                        continue
                        
                    ox1, oy1, ox2, oy2 = other_det['bbox']
                    
                    # è¨ˆç®—é‡ç–Šé¢ç©
                    overlap_x1 = max(pos_x1, ox1)
                    overlap_y1 = max(pos_y1, oy1)
                    overlap_x2 = min(pos_x2, ox2)
                    overlap_y2 = min(pos_y2, oy2)
                    
                    if overlap_x1 < overlap_x2 and overlap_y1 < overlap_y2:
                        overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)
                        box_area = (ox2 - ox1) * (oy2 - oy1)
                        
                        # å¦‚æœæ¨™ç±¤é®æ“‹å…¶ä»–æ¡†è¶…é 20%ï¼Œå‰‡ä¸åˆé©
                        if overlap_area > box_area * 0.2:
                            blocks_other_boxes = True
                            break
                
                if not blocks_other_boxes:
                    best_position = (pos_x1, pos_y1, pos_x2, pos_y2)
                    break
            
            # å‚™æ´ä½ç½® - å¦‚æœéƒ½ä¸è¡Œï¼Œå°‹æ‰¾åœ–åƒé‚Šç·£ç©ºç™½å€åŸŸ
            if best_position is None:
                # å˜—è©¦åœ–åƒå››å€‹è§’è½
                corner_candidates = [
                    # å·¦ä¸Šè§’
                    (5, 5, 5 + label_width, 5 + label_height),
                    # å³ä¸Šè§’  
                    (w - label_width - 5, 5, w - 5, 5 + label_height),
                    # å·¦ä¸‹è§’
                    (5, h - label_height - 5, 5 + label_width, h - 5),
                    # å³ä¸‹è§’
                    (w - label_width - 5, h - label_height - 5, w - 5, h - 5),
                ]
                
                for pos_x1, pos_y1, pos_x2, pos_y2 in corner_candidates:
                    # æª¢æŸ¥è§’è½ä½ç½®æ˜¯å¦èˆ‡å·²æœ‰æ¨™ç±¤é‡ç–Š
                    corner_overlap = False
                    for occupied in occupied_regions:
                        if not (pos_x2 < occupied[0] or pos_x1 > occupied[2] or 
                               pos_y2 < occupied[1] or pos_y1 > occupied[3]):
                            corner_overlap = True
                            break
                    
                    if not corner_overlap:
                        best_position = (pos_x1, pos_y1, pos_x2, pos_y2)
                        break
                
                # æœ€çµ‚å‚™æ´ï¼šå¼·åˆ¶æ”¾åœ¨æ¡†ä¸Šæ–¹ï¼ˆå³ä½¿å¯èƒ½é‡ç–Šï¼‰
                if best_position is None:
                    pos_x1 = max(5, min(x1, w - label_width - 5))
                    pos_y1 = max(label_height + 5, y1 - 10)
                    best_position = (pos_x1, pos_y1, pos_x1 + label_width, pos_y1 + label_height)
            
            # è¨˜éŒ„ä½ç½®
            label_positions.append((best_position[0], best_position[1] + label_height))
            occupied_regions.append(best_position)
        
        # é‡æ–°æ’åºåˆ°åŸå§‹é †åº
        final_positions: List[Optional[Tuple[int, int]]] = [None] * len(detections)
        for i, (original_idx, _) in enumerate(sorted_detections):
            final_positions[original_idx] = label_positions[i]
        
        return final_positions

    def annotate_image(self, image: Image.Image, detections: List[Dict]) -> Image.Image:
        """åœ¨åœ–åƒä¸Šæ¨™è¨»æª¢æ¸¬çµæœï¼ˆæ¡ç”¨æ™ºèƒ½æ¨™ç±¤å®šä½ï¼‰"""
        if not detections:
            return image
            
        # å‰µå»ºå‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸåœ–
        annotated = image.copy()
        draw = ImageDraw.Draw(annotated)
        
        # ç²å–æœ€ä½³å­—é«”
        font = self.get_optimal_font(FONT_SIZE)
        
        # è¨ˆç®—æ™ºèƒ½æ¨™ç±¤ä½ç½®
        label_positions = self.calculate_smart_label_positions(detections, image.size, FONT_SIZE)
        
        for i, det in enumerate(detections):
            bbox = det['bbox']
            x1, y1, x2, y2 = bbox
            class_name = det['class_name']
            confidence = det['confidence']
            
            # é¸æ“‡é¡è‰²
            color = COLORS[i % len(COLORS)]
            
            # ç¹ªè£½é‚Šç•Œæ¡†
            draw.rectangle([x1, y1, x2, y2], outline=color, width=BOX_THICKNESS)
            
            # æº–å‚™æ¨™ç±¤æ–‡å­—
            label = f"{class_name} {confidence:.2f}"
            
            # ä½¿ç”¨æ™ºèƒ½è¨ˆç®—çš„ä½ç½®
            if i < len(label_positions) and label_positions[i] is not None:
                position = label_positions[i]
                if position is not None:
                    text_x, text_y = position
                    text_y -= FONT_SIZE  # èª¿æ•´ç‚ºæ–‡å­—é ‚éƒ¨ä½ç½®
                else:
                    # å‚™æ´ä½ç½®
                    text_x = x1
                    text_y = y1 - FONT_SIZE - 5
                    if text_y < 0:
                        text_y = y1 + 5
            else:
                # å‚™æ´ä½ç½®
                text_x = x1
                text_y = y1 - FONT_SIZE - 5
                if text_y < 0:
                    text_y = y1 + 5
            
            # è¨ˆç®—æ–‡å­—å°ºå¯¸
            text_bbox = draw.textbbox((0, 0), label, font=font)
            text_width = text_bbox[2] - text_bbox[0]
            text_height = text_bbox[3] - text_bbox[1]
            
            padding = 4
            
            # ç¹ªè£½æ¨™ç±¤èƒŒæ™¯
            draw.rectangle(
                [text_x - padding, text_y - padding, 
                 text_x + text_width + padding, text_y + text_height + padding],
                fill=color, outline=color
            )
            
            # ç¹ªè£½æ¨™ç±¤æ–‡å­—
            draw.text((text_x, text_y), label, fill=(255, 255, 255), font=font)
            
        return annotated
        
    async def detect_from_url(self, url: str) -> Dict:
        """å¾åœ–ç‰‡ URL é€²è¡Œè—¥ä¸¸æª¢æ¸¬"""
        try:
            # ä¸‹è¼‰åœ–åƒ
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            # è¼‰å…¥ä¸¦è™•ç†åœ–åƒ
            image = Image.open(BytesIO(response.content))
            return await self._detect_from_image(image)
            
        except Exception as e:
            logger.error(f"âŒ URL æª¢æ¸¬å¤±æ•—: {e}")
            raise
            
    async def detect_from_file(self, file_content: bytes) -> Dict:
        """å¾ä¸Šå‚³æª”æ¡ˆå…§å®¹é€²è¡Œè—¥ä¸¸æª¢æ¸¬"""
        try:
            image = Image.open(BytesIO(file_content))
            return await self._detect_from_image(image)
            
        except Exception as e:
            logger.error(f"âŒ æª”æ¡ˆæª¢æ¸¬å¤±æ•—: {e}")
            raise
            
    async def _detect_from_image(self, image: Image.Image) -> Dict:
        """åŸ·è¡Œå–®å¼µåœ–ç‰‡çš„å®Œæ•´æª¢æ¸¬æµç¨‹"""
        # è½‰æ›ç‚º RGB æ¨¡å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
            
        original_size = image.size
        
        # è½‰æ›ç‚º numpy æ•¸çµ„
        image_array = np.array(image)
        
        # é è™•ç† (åŒ…å« resize)
        input_tensor = self.preprocess_image(image_array)
        
        # æ¨¡å‹æ¨ç†
        input_name = self.onnx_session.get_inputs()[0].name
        outputs = self.onnx_session.run(None, {input_name: input_tensor})
        
        # å¾Œè™•ç†
        detections = self.postprocess_results(outputs, original_size)
        
        # æ¨™è¨»åœ–åƒ
        annotated_image = self.annotate_image(image, detections)
        
        # è½‰æ›ç‚º base64
        buffer = BytesIO()
        annotated_image.save(buffer, format='JPEG', quality=95)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        return {
            'detections': detections,
            'annotated_image': f"data:image/jpeg;base64,{image_base64}",
            'total_detections': len(detections)
        }
        
    def get_classes(self) -> List[str]:
        """å–å¾—æ‰€æœ‰æ”¯æ´çš„è—¥ä¸¸é¡åˆ¥åç¨±"""
        return self.class_names or []
        
    def is_ready(self) -> bool:
        """æª¢æŸ¥æ¨¡å‹èˆ‡é¡åˆ¥åç¨±æ˜¯å¦è¼‰å…¥å®Œæˆ"""
        return self.onnx_session is not None and self.class_names is not None