from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, HttpUrl, Field
import onnxruntime as ort
import numpy as np
import cv2
from PIL import Image, ImageDraw, ImageFont
import time
import logging
import json
import os
from contextlib import asynccontextmanager
import io
import requests
from typing import List, Dict, Optional

# ğŸ”§ è¨­ç½®æ›´è©³ç´°çš„æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å…¨åŸŸè®Šæ•¸
onnx_session = None
class_names = None

# ğŸ¯ é…ç½®å¸¸æ•¸
class Config:
    MODEL_PATH = "./models/inference_model.onnx"
    ANNOTATIONS_PATH = "./app/_annotations.coco.json"
    INPUT_SIZE = (560, 560)
    DEFAULT_THRESHOLD = 0.5
    NUM_SELECT = 30
    
    # ImageNet æ­£è¦åŒ–åƒæ•¸
    IMAGENET_MEAN = [0.485, 0.456, 0.406]
    IMAGENET_STD = [0.229, 0.224, 0.225]
    
    # æª¢æ¸¬æ¡†é¡è‰²èª¿è‰²æ¿ (BGR æ ¼å¼)
    COLORS = [
        (0, 255, 0),      # äº®ç¶ è‰²
        (255, 0, 0),      # äº®è—è‰²  
        (0, 0, 255),      # äº®ç´…è‰²
        (0, 255, 255),    # äº®é»ƒè‰²
        (255, 0, 255),    # äº®ç´«è‰²
        (255, 165, 0),    # æ©™è‰²
        (0, 128, 255),    # æ·ºè—è‰²
        (128, 0, 128),    # ç´«è‰²
        (255, 192, 203),  # ç²‰ç´…è‰²
        (0, 255, 127),    # æ˜¥ç¶ è‰²
    ]

def load_class_names_from_coco() -> Optional[List[str]]:
    """å¾ COCO annotations æ–‡ä»¶è¼‰å…¥é¡åˆ¥åç¨±"""
    try:
        if not os.path.exists(Config.ANNOTATIONS_PATH):
            logger.error(f"âŒ COCO æ¨™è¨»æ–‡ä»¶ä¸å­˜åœ¨: {Config.ANNOTATIONS_PATH}")
            return None
            
        with open(Config.ANNOTATIONS_PATH, "r", encoding='utf-8') as f:
            coco_data = json.load(f)
        
        # æŒ‰ç…§ category_id æ’åºï¼Œç¢ºä¿ç´¢å¼•æ­£ç¢º
        categories = sorted(coco_data['categories'], key=lambda x: x['id'])
        class_names = [cat['name'] for cat in categories]
        
        logger.info(f"âœ… æˆåŠŸè¼‰å…¥ {len(class_names)} å€‹é¡åˆ¥")
        logger.info(f"ğŸ“‹ é¡åˆ¥åˆ—è¡¨: {class_names}")
        
        return class_names
        
    except Exception as e:
        logger.error(f"âŒ è¼‰å…¥ COCO æ¨™è¨»æ–‡ä»¶å¤±æ•—: {e}")
        return None

def preprocess_image_for_onnx(image_array: np.ndarray, input_size: tuple = Config.INPUT_SIZE) -> np.ndarray:
    """
    ç‚º ONNX æ¨¡å‹é è™•ç†åœ–åƒ - å®Œå…¨åŒ¹é… PyTorch è™•ç†æµç¨‹
    
    Args:
        image_array: RGB æ ¼å¼çš„åœ–åƒæ•¸çµ„
        input_size: ç›®æ¨™è¼¸å…¥å°ºå¯¸ (width, height)
    
    Returns:
        é è™•ç†å¾Œçš„å¼µé‡ (1, 3, H, W)
    """
    try:
        # æ­¥é©Ÿ1: F.to_tensor() - è½‰æ›ç‚º CHW ä¸¦æ­£è¦åŒ–åˆ° [0,1]
        tensor_like = image_array.transpose((2, 0, 1)).astype(np.float32) / 255.0
        
        # æ­¥é©Ÿ2: F.normalize() - ImageNet æ­£è¦åŒ–
        means = np.array(Config.IMAGENET_MEAN, dtype=np.float32).reshape(-1, 1, 1)
        stds = np.array(Config.IMAGENET_STD, dtype=np.float32).reshape(-1, 1, 1)
        normalized = (tensor_like - means) / stds
        
        # æ­¥é©Ÿ3: F.resize() - èª¿æ•´åˆ°æ¨¡å‹è¼¸å…¥å°ºå¯¸
        hwc_normalized = normalized.transpose((1, 2, 0))  # CHW -> HWC for OpenCV
        resized_hwc = cv2.resize(hwc_normalized, input_size, interpolation=cv2.INTER_LINEAR)
        resized_chw = resized_hwc.transpose((2, 0, 1))  # HWC -> CHW
        
        # æ­¥é©Ÿ4: æ·»åŠ  batch ç¶­åº¦
        batched = np.expand_dims(resized_chw, axis=0)
        
        logger.debug(f"ğŸ”§ é è™•ç†çµ±è¨ˆ: ç¯„åœ[{np.min(batched):.3f}, {np.max(batched):.3f}], å‡å€¼{np.mean(batched):.3f}")
        
        return batched
        
    except Exception as e:
        logger.error(f"âŒ åœ–åƒé è™•ç†å¤±æ•—: {e}")
        raise

def postprocess_onnx_output(
    outputs: List[np.ndarray], 
    threshold: float = Config.DEFAULT_THRESHOLD,
    target_size: tuple = Config.INPUT_SIZE,
    num_select: int = Config.NUM_SELECT
) -> Dict[str, np.ndarray]:
    """
    å¾Œè™•ç† ONNX æ¨¡å‹è¼¸å‡º - RF-DETR å¾Œè™•ç†é‚è¼¯
    
    Args:
        outputs: ONNX æ¨¡å‹è¼¸å‡º [boxes, logits]
        threshold: ä¿¡å¿ƒåº¦é–¾å€¼
        target_size: ç›®æ¨™åœ–åƒå°ºå¯¸ (width, height)
        num_select: Top-K é¸æ“‡æ•¸é‡
    
    Returns:
        æª¢æ¸¬çµæœå­—å…¸
    """
    try:
        pred_boxes, pred_logits = outputs[0][0], outputs[1][0]  # ç§»é™¤ batch ç¶­åº¦
        
        # Sigmoid æ¿€æ´»
        prob = 1.0 / (1.0 + np.exp(-pred_logits))
        
        # Top-K é¸æ“‡
        prob_flat = prob.reshape(-1)
        topk_indices = np.argpartition(prob_flat, -num_select)[-num_select:]
        topk_indices = topk_indices[np.argsort(prob_flat[topk_indices])[::-1]]
        
        scores = prob_flat[topk_indices]
        topk_boxes = topk_indices // pred_logits.shape[1]
        labels = topk_indices % pred_logits.shape[1]
        
        # é‚Šç•Œæ¡†æ ¼å¼è½‰æ› cxcywh -> xyxy
        def box_cxcywh_to_xyxy(boxes):
            cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
            x1, y1 = cx - 0.5 * w, cy - 0.5 * h
            x2, y2 = cx + 0.5 * w, cy + 0.5 * h
            return np.stack([x1, y1, x2, y2], axis=1)
        
        boxes_xyxy = box_cxcywh_to_xyxy(pred_boxes)
        selected_boxes = boxes_xyxy[topk_boxes]
        
        # ç¸®æ”¾åˆ°ç›®æ¨™å°ºå¯¸
        img_w, img_h = target_size
        scale_fct = np.array([img_w, img_h, img_w, img_h])
        final_boxes = selected_boxes * scale_fct
        
        # æ‡‰ç”¨é–¾å€¼éæ¿¾
        valid_mask = scores >= threshold
        if not np.any(valid_mask):
            logger.info("â„¹ï¸ æ²’æœ‰æª¢æ¸¬çµæœè¶…éé–¾å€¼")
            return {
                'xyxy': np.array([]).reshape(0, 4),
                'confidence': np.array([]),
                'class_id': np.array([])
            }
        
        filtered_boxes = final_boxes[valid_mask]
        filtered_scores = scores[valid_mask]
        filtered_labels = labels[valid_mask]
        
        # åº§æ¨™ç¯„åœé™åˆ¶
        filtered_boxes[:, [0, 2]] = np.clip(filtered_boxes[:, [0, 2]], 0, img_w)
        filtered_boxes[:, [1, 3]] = np.clip(filtered_boxes[:, [1, 3]], 0, img_h)
        
        logger.info(f"âœ… æª¢æ¸¬åˆ° {len(filtered_boxes)} å€‹å°è±¡")
        
        return {
            'xyxy': filtered_boxes,
            'confidence': filtered_scores,
            'class_id': filtered_labels
        }
        
    except Exception as e:
        logger.error(f"âŒ å¾Œè™•ç†å¤±æ•—: {e}")
        raise

def get_optimal_font(font_size: int) -> ImageFont.ImageFont:
    """ç²å–æœ€ä½³å¯ç”¨å­—é«”"""
    font_paths = [
        "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf",
        "/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf",
        "/usr/share/fonts/TTF/DejaVuSans-Bold.ttf",
        "/System/Library/Fonts/Helvetica.ttc",
        "C:/Windows/Fonts/arialbd.ttf",
    ]
    
    for font_path in font_paths:
        try:
            return ImageFont.truetype(font_path, font_size)
        except:
            continue
    
    return ImageFont.load_default()

def draw_high_quality_text(
    image_array: np.ndarray, 
    text: str, 
    position: tuple, 
    font_size: int, 
    bg_color: tuple
) -> np.ndarray:
    """ä½¿ç”¨ PIL ç¹ªè£½é«˜è³ªé‡æ–‡å­—"""
    try:
        # è½‰æ›åˆ° PIL
        image_rgb = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(image_rgb)
        draw = ImageDraw.Draw(pil_image)
        
        # ç²å–å­—é«”
        font = get_optimal_font(font_size)
        
        # è¨ˆç®—æ–‡å­—å°ºå¯¸
        bbox = draw.textbbox((0, 0), text, font=font)
        text_width, text_height = bbox[2] - bbox[0], bbox[3] - bbox[1]
        
        x, y = position
        padding = 4
        
        # ç¹ªè£½èƒŒæ™¯
        bg_bbox = [
            x - padding,
            y - text_height - padding,
            x + text_width + padding,
            y + padding
        ]
        
        bg_color_rgb = (bg_color[2], bg_color[1], bg_color[0])  # BGR -> RGB
        draw.rectangle(bg_bbox, fill=bg_color_rgb)
        
        # ç¹ªè£½æ–‡å­—
        draw.text((x, y - text_height), text, font=font, fill=(255, 255, 255))
        
        # è½‰æ›å› OpenCV æ ¼å¼
        result_rgb = np.array(pil_image)
        return cv2.cvtColor(result_rgb, cv2.COLOR_RGB2BGR)
        
    except Exception as e:
        logger.error(f"âŒ æ–‡å­—ç¹ªè£½å¤±æ•—: {e}")
        return image_array

def calculate_smart_label_positions(
    detections: Dict[str, np.ndarray], 
    image_size: tuple, 
    font_size: int
) -> List[tuple]:
    """æ™ºèƒ½è¨ˆç®—æ¨™ç±¤ä½ç½®ï¼Œé¿å…é‡ç–Šå’Œé®æ“‹æª¢æ¸¬æ¡†"""
    w, h = image_size
    boxes = detections['xyxy']
    confidences = detections['confidence']
    
    if len(boxes) == 0:
        return []
    
    label_height = font_size + 8
    max_label_width = font_size * 12
    
    label_positions = []
    occupied_regions = []
    
    # æŒ‰ä¿¡å¿ƒåº¦æ’åº
    sorted_indices = np.argsort(confidences)[::-1]
    
    for idx in sorted_indices:
        box = boxes[idx]
        class_id = detections['class_id'][idx]
        conf = confidences[idx]
        
        pill_name = class_names[class_id] if class_id < len(class_names) else f"Unknown_{class_id}"
        label_text = f"{pill_name} {conf:.2f}"
        label_width = min(len(label_text) * font_size * 0.6, max_label_width)
        
        x1, y1, x2, y2 = box.astype(int)
        
        # å€™é¸ä½ç½® - å¢åŠ æ›´å¤šé¸é …
        candidates = [
            # ä¸Šæ–¹ - å¤šå€‹ä½ç½®
            (x1, y1 - label_height - 5, x1 + label_width, y1 - 5),
            (x1 + (x2-x1)//4, y1 - label_height - 5, x1 + (x2-x1)//4 + label_width, y1 - 5),
            (max(0, x2 - label_width), y1 - label_height - 5, x2, y1 - 5),
            
            # ä¸‹æ–¹ - å¤šå€‹ä½ç½®  
            (x1, y2 + 5, x1 + label_width, y2 + 5 + label_height),
            (x1 + (x2-x1)//4, y2 + 5, x1 + (x2-x1)//4 + label_width, y2 + 5 + label_height),
            (max(0, x2 - label_width), y2 + 5, x2, y2 + 5 + label_height),
            
            # å·¦å´
            (x1 - label_width - 5, y1, x1 - 5, y1 + label_height),
            (x1 - label_width - 5, y1 + (y2-y1)//4, x1 - 5, y1 + (y2-y1)//4 + label_height),
            
            # å³å´
            (x2 + 5, y1, x2 + 5 + label_width, y1 + label_height),
            (x2 + 5, y1 + (y2-y1)//4, x2 + 5 + label_width, y1 + (y2-y1)//4 + label_height),
            
            # æ¡†å…§ä¸Šæ–¹
            (x1 + 5, y1 + 5, x1 + 5 + label_width, y1 + 5 + label_height),
            # æ¡†å…§ä¸‹æ–¹
            (x1 + 5, y2 - label_height - 5, x1 + 5 + label_width, y2 - 5),
        ]
        
        # æ‰¾æœ€ä½³ä½ç½®
        best_position = None
        for pos_x1, pos_y1, pos_x2, pos_y2 in candidates:
            # 1. é‚Šç•Œæª¢æŸ¥
            if pos_x1 < 0 or pos_y1 < 0 or pos_x2 > w or pos_y2 > h:
                continue
            
            # 2. èˆ‡å…¶ä»–æ¨™ç±¤é‡ç–Šæª¢æŸ¥
            overlaps_label = False
            for occupied in occupied_regions:
                if not (pos_x2 < occupied[0] or pos_x1 > occupied[2] or 
                       pos_y2 < occupied[1] or pos_y1 > occupied[3]):
                    overlaps_label = True
                    break
            
            if overlaps_label:
                continue
            
            # 3. ğŸ” æª¢æŸ¥æ˜¯å¦é®æ“‹å…¶ä»–æª¢æ¸¬æ¡† (é€™æ˜¯é—œéµï¼)
            blocks_other_boxes = False
            for other_idx in range(len(boxes)):
                if other_idx == idx:  # è·³éè‡ªå·±
                    continue
                    
                other_box = boxes[other_idx]
                ox1, oy1, ox2, oy2 = other_box.astype(int)
                
                # è¨ˆç®—é‡ç–Šé¢ç©
                overlap_x1 = max(pos_x1, ox1)
                overlap_y1 = max(pos_y1, oy1)
                overlap_x2 = min(pos_x2, ox2)
                overlap_y2 = min(pos_y2, oy2)
                
                if overlap_x1 < overlap_x2 and overlap_y1 < overlap_y2:
                    overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)
                    box_area = (ox2 - ox1) * (oy2 - oy1)
                    
                    # ğŸ¯ å¦‚æœæ¨™ç±¤é®æ“‹å…¶ä»–æ¡†è¶…é 20%ï¼Œå‰‡ä¸åˆé©
                    if overlap_area > box_area * 0.2:  # å¾ 30% é™åˆ° 20% æ›´åš´æ ¼
                        blocks_other_boxes = True
                        break
            
            if not blocks_other_boxes:
                best_position = (pos_x1, pos_y1, pos_x2, pos_y2)
                break
        
        # 4. ğŸš¨ å‚™æ´ä½ç½® - å¦‚æœéƒ½ä¸è¡Œï¼Œå°‹æ‰¾åœ–åƒé‚Šç·£ç©ºç™½å€åŸŸ
        if best_position is None:
            # å˜—è©¦åœ–åƒå››å€‹è§’è½
            corner_candidates = [
                # å·¦ä¸Šè§’
                (5, 5, 5 + label_width, 5 + label_height),
                # å³ä¸Šè§’  
                (w - label_width - 5, 5, w - 5, 5 + label_height),
                # å·¦ä¸‹è§’
                (5, h - label_height - 5, 5 + label_width, h - 5),
                # å³ä¸‹è§’
                (w - label_width - 5, h - label_height - 5, w - 5, h - 5),
            ]
            
            for pos_x1, pos_y1, pos_x2, pos_y2 in corner_candidates:
                # æª¢æŸ¥è§’è½ä½ç½®æ˜¯å¦èˆ‡å·²æœ‰æ¨™ç±¤é‡ç–Š
                corner_overlap = False
                for occupied in occupied_regions:
                    if not (pos_x2 < occupied[0] or pos_x1 > occupied[2] or 
                           pos_y2 < occupied[1] or pos_y1 > occupied[3]):
                        corner_overlap = True
                        break
                
                if not corner_overlap:
                    best_position = (pos_x1, pos_y1, pos_x2, pos_y2)
                    break
            
            # 5. ğŸ”§ æœ€çµ‚å‚™æ´ï¼šå¼·åˆ¶æ”¾åœ¨æ¡†ä¸Šæ–¹ï¼ˆå³ä½¿å¯èƒ½é‡ç–Šï¼‰
            if best_position is None:
                pos_x1 = max(5, min(x1, w - label_width - 5))
                pos_y1 = max(label_height + 5, y1 - 10)
                best_position = (pos_x1, pos_y1, pos_x1 + label_width, pos_y1 + label_height)
        
        # è¨˜éŒ„ä½ç½®
        label_positions.append((best_position[0], best_position[1] + label_height))
        occupied_regions.append(best_position)
    
    # é‡æ–°æ’åºåˆ°åŸå§‹é †åº
    final_positions = [None] * len(boxes)
    for i, idx in enumerate(sorted_indices):
        final_positions[idx] = label_positions[i]
    
    return final_positions

def save_image_to_base64(image: Image.Image) -> str:
    """å°‡åœ–ç‰‡è½‰ç‚º base64"""
    buffer = io.BytesIO()
    image.save(buffer, format='JPEG', quality=90)
    import base64
    return base64.b64encode(buffer.getvalue()).decode()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ‡‰ç”¨ç”Ÿå‘½é€±æœŸç®¡ç†"""
    global onnx_session, class_names
    
    try:
        logger.info("ğŸš€ å•Ÿå‹•è—¥ä¸¸æª¢æ¸¬ API...")
        
        # è¼‰å…¥é¡åˆ¥åç¨±
        class_names = load_class_names_from_coco()
        if class_names is None:
            raise RuntimeError("ç„¡æ³•è¼‰å…¥é¡åˆ¥åç¨±")
        
        # è¼‰å…¥ ONNX æ¨¡å‹
        if not os.path.exists(Config.MODEL_PATH):
            raise RuntimeError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {Config.MODEL_PATH}")
        
        logger.info(f"ğŸ“¦ è¼‰å…¥ ONNX æ¨¡å‹: {Config.MODEL_PATH}")
        onnx_session = ort.InferenceSession(
            Config.MODEL_PATH,
            providers=['CPUExecutionProvider']
        )
        
        # æ¨¡å‹ä¿¡æ¯
        input_info = onnx_session.get_inputs()[0]
        output_info = onnx_session.get_outputs()
        
        logger.info(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        logger.info(f"ğŸ“¥ è¼¸å…¥: {input_info.name}{input_info.shape}")
        logger.info(f"ğŸ“¤ è¼¸å‡º: {[out.name + str(out.shape) for out in output_info]}")
        logger.info(f"ğŸ¯ æ”¯æ´ {len(class_names)} ç¨®è—¥ä¸¸")
        logger.info("ğŸŒŸ API æº–å‚™å°±ç·’ï¼")
        
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–å¤±æ•—: {e}")
        raise
    
    yield
    
    logger.info("ğŸ›‘ é—œé–‰è—¥ä¸¸æª¢æ¸¬ API")
    onnx_session = None
    class_names = None

# FastAPI æ‡‰ç”¨
app = FastAPI(
    title="ğŸ’Š Pill Detection API",
    description="AI è—¥ä¸¸è­˜åˆ¥æœå‹™ - ä½¿ç”¨ RF-DETR æ¨¡å‹é€²è¡Œé«˜ç²¾åº¦è—¥ä¸¸æª¢æ¸¬",
    version="2.1.0",
    lifespan=lifespan
)

# CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è«‹æ±‚/å›æ‡‰æ¨¡å‹
class DetectionRequest(BaseModel):
    image_url: HttpUrl = Field(..., description="åœ–åƒ URL")
    threshold: float = Field(
        default=Config.DEFAULT_THRESHOLD, 
        ge=0.1, le=1.0, 
        description="æª¢æ¸¬ä¿¡å¿ƒåº¦é–¾å€¼ (0.1-1.0)"
    )

class DetectionResult(BaseModel):
    detection_id: int
    class_id: int
    pill_name: str
    confidence: float
    bbox: List[float]

class DetectionResponse(BaseModel):
    success: bool
    detections: List[DetectionResult]
    annotated_image_base64: str
    inference_time_ms: float
    total_detections: int

# API ç«¯é»
@app.get("/", tags=["ç³»çµ±"])
async def root():
    """API æ ¹ç«¯é»"""
    return {
        "name": "ğŸ’Š Pill Detection API",
        "version": "2.1.0",
        "status": "running",
        "model": "RF-DETR ONNX",
        "supported_pills": len(class_names) if class_names else 0
    }

@app.get("/health", tags=["ç³»çµ±"])
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    return {
        "status": "healthy",
        "model_loaded": onnx_session is not None,
        "classes_loaded": class_names is not None,
        "total_classes": len(class_names) if class_names else 0,
        "memory_usage": "OK",
        "timestamp": time.time()
    }

@app.get("/classes", tags=["æ¨¡å‹"])
async def get_supported_classes():
    """ç²å–æ‰€æœ‰æ”¯æ´çš„è—¥ä¸¸é¡åˆ¥"""
    if not class_names:
        raise HTTPException(status_code=503, detail="é¡åˆ¥åç¨±æœªè¼‰å…¥")
    
    return {
        "total_classes": len(class_names),
        "classes": [
            {"id": i, "name": name} 
            for i, name in enumerate(class_names)
        ]
    }

@app.post("/detect", response_model=DetectionResponse, tags=["æª¢æ¸¬"])
async def detect_pills(request: DetectionRequest):
    """
    è—¥ä¸¸æª¢æ¸¬ä¸»ç«¯é»
    
    ä¸Šå‚³åœ–åƒ URLï¼Œè¿”å›æª¢æ¸¬åˆ°çš„è—¥ä¸¸ä¿¡æ¯å’Œæ¨™è¨»åœ–åƒ
    """
    if not onnx_session or not class_names:
        raise HTTPException(status_code=503, detail="æœå‹™æœªå°±ç·’")
    
    try:
        # ğŸ”½ ä¸‹è¼‰åœ–åƒ
        logger.info(f"ğŸ“¥ ä¸‹è¼‰åœ–åƒ: {request.image_url}")
        response = requests.get(str(request.image_url), timeout=15)
        response.raise_for_status()
        
        pil_image = Image.open(io.BytesIO(response.content)).convert('RGB')
        orig_w, orig_h = pil_image.size
        logger.info(f"ğŸ–¼ï¸ åœ–åƒå°ºå¯¸: {orig_w}x{orig_h}")
        
        # ğŸ”„ é è™•ç†
        image_array = np.array(pil_image)
        input_tensor = preprocess_image_for_onnx(image_array)
        
        # ğŸ¤– æ¨ç†
        start_time = time.time()
        input_name = onnx_session.get_inputs()[0].name
        outputs = onnx_session.run(None, {input_name: input_tensor})
        inference_time = (time.time() - start_time) * 1000
        
        # ğŸ” å¾Œè™•ç†
        detections = postprocess_onnx_output(
            outputs, 
            threshold=request.threshold,
            target_size=(orig_w, orig_h)
        )
        
        total_detections = len(detections['class_id'])
        logger.info(f"ğŸ¯ æª¢æ¸¬åˆ° {total_detections} å€‹è—¥ä¸¸")
        
        # ğŸ¨ ç¹ªè£½çµæœ - ä¿®æ­£ç¹ªè£½é †åº
        annotated_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
        
        if total_detections > 0:
            h, w = image_array.shape[:2]
            font_size = max(16, int(min(w, h) / 35))
            thickness = max(2, int(min(w, h) / 400))
            
            # æ™ºèƒ½æ¨™ç±¤ä½ç½®
            label_positions = calculate_smart_label_positions(detections, (w, h), font_size)
            
            # ğŸ¯ æ­¥é©Ÿ1: å…ˆç¹ªè£½æ‰€æœ‰æª¢æ¸¬æ¡†
            for i, (box, conf, class_id) in enumerate(zip(
                detections['xyxy'], 
                detections['confidence'], 
                detections['class_id']
            )):
                x1, y1, x2, y2 = box.astype(int)
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(w, x2), min(h, y2)
                
                color = Config.COLORS[class_id % len(Config.COLORS)]
                
                # åªç¹ªè£½é‚Šç•Œæ¡†ï¼Œä¸ç¹ªè£½æ¨™ç±¤
                cv2.rectangle(annotated_array, (x1, y1), (x2, y2), color, thickness)
            
            # ğŸ·ï¸ æ­¥é©Ÿ2: å†ç¹ªè£½æ‰€æœ‰æ¨™ç±¤ (é€™æ¨£æ¨™ç±¤æœƒåœ¨æ¡†çš„ä¸Šå±¤)
            for i, (box, conf, class_id) in enumerate(zip(
                detections['xyxy'], 
                detections['confidence'], 
                detections['class_id']
            )):
                pill_name = class_names[class_id] if class_id < len(class_names) else f"Unknown_{class_id}"
                color = Config.COLORS[class_id % len(Config.COLORS)]
                
                # ç¹ªè£½æ¨™ç±¤
                label = f"{pill_name} {conf:.2f}"
                text_x, text_y = label_positions[i]
                annotated_array = draw_high_quality_text(
                    annotated_array, label, (text_x, text_y), font_size, color
                )
        
        # ğŸ“¤ è¼¸å‡ºçµæœ
        annotated_pil = Image.fromarray(cv2.cvtColor(annotated_array, cv2.COLOR_BGR2RGB))
        image_base64 = save_image_to_base64(annotated_pil)
        
        detection_results = [
            DetectionResult(
                detection_id=i + 1,
                class_id=int(class_id),
                pill_name=class_names[class_id] if class_id < len(class_names) else f"Unknown_{class_id}",
                confidence=float(conf),
                bbox=[float(x) for x in box]
            )
            for i, (class_id, conf, box) in enumerate(zip(
                detections['class_id'], 
                detections['confidence'], 
                detections['xyxy']
            ))
        ]
        
        logger.info(f"âœ… è™•ç†å®Œæˆ - æª¢æ¸¬:{total_detections}, æ¨ç†:{inference_time:.1f}ms")
        
        return DetectionResponse(
            success=True,
            detections=detection_results,
            annotated_image_base64=image_base64,
            inference_time_ms=round(inference_time, 2),
            total_detections=total_detections
        )
        
    except requests.RequestException as e:
        logger.error(f"âŒ åœ–åƒä¸‹è¼‰å¤±æ•—: {e}")
        raise HTTPException(status_code=400, detail="ç„¡æ³•ä¸‹è¼‰åœ–åƒ")
    except Exception as e:
        logger.error(f"âŒ æª¢æ¸¬å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"æª¢æ¸¬è™•ç†å¤±æ•—: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app", 
        host="0.0.0.0", 
        port=8000, 
        reload=True,
        log_level="info"
    )