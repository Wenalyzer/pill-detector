from fastapi import FastAPI, HTTPException, Request, File, UploadFile, Form
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
from pydantic import BaseModel, HttpUrl, Field
import onnxruntime as ort
import numpy as np
import cv2
from PIL import Image, ImageDraw, ImageFont
import time
import logging
import json
import os
from contextlib import asynccontextmanager
import io
import requests
from typing import List, Dict, Optional

# ğŸ”§ è¨­ç½®æ›´è©³ç´°çš„æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å…¨åŸŸè®Šæ•¸
onnx_session = None
class_names = None

# ğŸ¯ é…ç½®å¸¸æ•¸
class Config:
    MODEL_PATH = "./models/inference_model.onnx"
    ANNOTATIONS_PATH = "./app/_annotations.coco.json"
    INPUT_SIZE = (560, 560)
    DEFAULT_THRESHOLD = 0.5
    NUM_SELECT = 30
    
    # ImageNet æ­£è¦åŒ–åƒæ•¸
    IMAGENET_MEAN = [0.485, 0.456, 0.406]
    IMAGENET_STD = [0.229, 0.224, 0.225]
    
    # æª¢æ¸¬æ¡†é¡è‰²èª¿è‰²æ¿ (BGR æ ¼å¼)
    COLORS = [
        (0, 255, 0),      # äº®ç¶ è‰²
        (255, 0, 0),      # äº®è—è‰²  
        (0, 0, 255),      # äº®ç´…è‰²
        (0, 255, 255),    # äº®é»ƒè‰²
        (255, 0, 255),    # äº®ç´«è‰²
        (255, 165, 0),    # æ©™è‰²
        (0, 128, 255),    # æ·ºè—è‰²
        (128, 0, 128),    # ç´«è‰²
        (255, 192, 203),  # ç²‰ç´…è‰²
        (0, 255, 127),    # æ˜¥ç¶ è‰²
    ]

def load_class_names_from_coco() -> Optional[List[str]]:
    """å¾ COCO annotations æ–‡ä»¶è¼‰å…¥é¡åˆ¥åç¨±"""
    try:
        if not os.path.exists(Config.ANNOTATIONS_PATH):
            logger.error(f"âŒ COCO æ¨™è¨»æ–‡ä»¶ä¸å­˜åœ¨: {Config.ANNOTATIONS_PATH}")
            return None
            
        with open(Config.ANNOTATIONS_PATH, "r", encoding='utf-8') as f:
            coco_data = json.load(f)
        
        # æŒ‰ç…§ category_id æ’åºï¼Œç¢ºä¿ç´¢å¼•æ­£ç¢º
        categories = sorted(coco_data['categories'], key=lambda x: x['id'])
        class_names = [cat['name'] for cat in categories]
        
        logger.info(f"âœ… æˆåŠŸè¼‰å…¥ {len(class_names)} å€‹é¡åˆ¥")
        logger.info(f"ğŸ“‹ é¡åˆ¥åˆ—è¡¨: {class_names}")
        
        return class_names
        
    except Exception as e:
        logger.error(f"âŒ è¼‰å…¥ COCO æ¨™è¨»æ–‡ä»¶å¤±æ•—: {e}")
        return None

def preprocess_image_for_onnx(image_array: np.ndarray, input_size: tuple = Config.INPUT_SIZE) -> np.ndarray:
    """
    ç‚º ONNX æ¨¡å‹é è™•ç†åœ–åƒ - å®Œå…¨åŒ¹é… PyTorch è™•ç†æµç¨‹
    
    Args:
        image_array: RGB æ ¼å¼çš„åœ–åƒæ•¸çµ„
        input_size: ç›®æ¨™è¼¸å…¥å°ºå¯¸ (width, height)
    
    Returns:
        é è™•ç†å¾Œçš„å¼µé‡ (1, 3, H, W)
    """
    try:
        # æ­¥é©Ÿ1: F.to_tensor() - è½‰æ›ç‚º CHW ä¸¦æ­£è¦åŒ–åˆ° [0,1]
        tensor_like = image_array.transpose((2, 0, 1)).astype(np.float32) / 255.0
        
        # æ­¥é©Ÿ2: F.normalize() - ImageNet æ­£è¦åŒ–
        means = np.array(Config.IMAGENET_MEAN, dtype=np.float32).reshape(-1, 1, 1)
        stds = np.array(Config.IMAGENET_STD, dtype=np.float32).reshape(-1, 1, 1)
        normalized = (tensor_like - means) / stds
        
        # æ­¥é©Ÿ3: F.resize() - èª¿æ•´åˆ°æ¨¡å‹è¼¸å…¥å°ºå¯¸
        hwc_normalized = normalized.transpose((1, 2, 0))  # CHW -> HWC for OpenCV
        resized_hwc = cv2.resize(hwc_normalized, input_size, interpolation=cv2.INTER_LINEAR)
        resized_chw = resized_hwc.transpose((2, 0, 1))  # HWC -> CHW
        
        # æ­¥é©Ÿ4: æ·»åŠ  batch ç¶­åº¦
        batched = np.expand_dims(resized_chw, axis=0)
        
        logger.debug(f"ğŸ”§ é è™•ç†çµ±è¨ˆ: ç¯„åœ[{np.min(batched):.3f}, {np.max(batched):.3f}], å‡å€¼{np.mean(batched):.3f}")
        
        return batched
        
    except Exception as e:
        logger.error(f"âŒ åœ–åƒé è™•ç†å¤±æ•—: {e}")
        raise

def postprocess_onnx_output(
    outputs: List[np.ndarray], 
    threshold: float = Config.DEFAULT_THRESHOLD,
    target_size: tuple = Config.INPUT_SIZE,
    num_select: int = Config.NUM_SELECT
) -> Dict[str, np.ndarray]:
    """
    å¾Œè™•ç† ONNX æ¨¡å‹è¼¸å‡º - RF-DETR å¾Œè™•ç†é‚è¼¯
    
    Args:
        outputs: ONNX æ¨¡å‹è¼¸å‡º [boxes, logits]
        threshold: ä¿¡å¿ƒåº¦é–¾å€¼
        target_size: ç›®æ¨™åœ–åƒå°ºå¯¸ (width, height)
        num_select: Top-K é¸æ“‡æ•¸é‡
    
    Returns:
        æª¢æ¸¬çµæœå­—å…¸
    """
    try:
        pred_boxes, pred_logits = outputs[0][0], outputs[1][0]  # ç§»é™¤ batch ç¶­åº¦
        
        # Sigmoid æ¿€æ´»
        prob = 1.0 / (1.0 + np.exp(-pred_logits))
        
        # Top-K é¸æ“‡
        prob_flat = prob.reshape(-1)
        topk_indices = np.argpartition(prob_flat, -num_select)[-num_select:]
        topk_indices = topk_indices[np.argsort(prob_flat[topk_indices])[::-1]]
        
        scores = prob_flat[topk_indices]
        topk_boxes = topk_indices // pred_logits.shape[1]
        labels = topk_indices % pred_logits.shape[1]
        
        # é‚Šç•Œæ¡†æ ¼å¼è½‰æ› cxcywh -> xyxy
        def box_cxcywh_to_xyxy(boxes):
            cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
            x1, y1 = cx - 0.5 * w, cy - 0.5 * h
            x2, y2 = cx + 0.5 * w, cy + 0.5 * h
            return np.stack([x1, y1, x2, y2], axis=1)
        
        boxes_xyxy = box_cxcywh_to_xyxy(pred_boxes)
        selected_boxes = boxes_xyxy[topk_boxes]
        
        # ç¸®æ”¾åˆ°ç›®æ¨™å°ºå¯¸
        img_w, img_h = target_size
        scale_fct = np.array([img_w, img_h, img_w, img_h])
        final_boxes = selected_boxes * scale_fct
        
        # æ‡‰ç”¨é–¾å€¼éæ¿¾
        valid_mask = scores >= threshold
        if not np.any(valid_mask):
            logger.info("â„¹ï¸ æ²’æœ‰æª¢æ¸¬çµæœè¶…éé–¾å€¼")
            return {
                'xyxy': np.array([]).reshape(0, 4),
                'confidence': np.array([]),
                'class_id': np.array([])
            }
        
        filtered_boxes = final_boxes[valid_mask]
        filtered_scores = scores[valid_mask]
        filtered_labels = labels[valid_mask]
        
        # åº§æ¨™ç¯„åœé™åˆ¶
        filtered_boxes[:, [0, 2]] = np.clip(filtered_boxes[:, [0, 2]], 0, img_w)
        filtered_boxes[:, [1, 3]] = np.clip(filtered_boxes[:, [1, 3]], 0, img_h)
        
        logger.info(f"âœ… æª¢æ¸¬åˆ° {len(filtered_boxes)} å€‹å°è±¡")
        
        return {
            'xyxy': filtered_boxes,
            'confidence': filtered_scores,
            'class_id': filtered_labels
        }
        
    except Exception as e:
        logger.error(f"âŒ å¾Œè™•ç†å¤±æ•—: {e}")
        raise

def get_optimal_font(font_size: int) -> ImageFont.ImageFont:
    """ç²å–æœ€ä½³å¯ç”¨å­—é«”"""
    font_paths = [
        "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf",
        "/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf",
        "/usr/share/fonts/TTF/DejaVuSans-Bold.ttf",
        "/System/Library/Fonts/Helvetica.ttc",
        "C:/Windows/Fonts/arialbd.ttf",
    ]
    
    for font_path in font_paths:
        try:
            return ImageFont.truetype(font_path, font_size)
        except:
            continue
    
    return ImageFont.load_default()

def draw_high_quality_text(
    image_array: np.ndarray, 
    text: str, 
    position: tuple, 
    font_size: int, 
    bg_color: tuple
) -> np.ndarray:
    """ä½¿ç”¨ PIL ç¹ªè£½é«˜è³ªé‡æ–‡å­—"""
    try:
        # è½‰æ›åˆ° PIL
        image_rgb = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(image_rgb)
        draw = ImageDraw.Draw(pil_image)
        
        # ç²å–å­—é«”
        font = get_optimal_font(font_size)
        
        # è¨ˆç®—æ–‡å­—å°ºå¯¸
        bbox = draw.textbbox((0, 0), text, font=font)
        text_width, text_height = bbox[2] - bbox[0], bbox[3] - bbox[1]
        
        x, y = position
        padding = 4
        
        # ç¹ªè£½èƒŒæ™¯
        bg_bbox = [
            x - padding,
            y - text_height - padding,
            x + text_width + padding,
            y + padding
        ]
        
        bg_color_rgb = (bg_color[2], bg_color[1], bg_color[0])  # BGR -> RGB
        draw.rectangle(bg_bbox, fill=bg_color_rgb)
        
        # ç¹ªè£½æ–‡å­—
        draw.text((x, y - text_height), text, font=font, fill=(255, 255, 255))
        
        # è½‰æ›å› OpenCV æ ¼å¼
        result_rgb = np.array(pil_image)
        return cv2.cvtColor(result_rgb, cv2.COLOR_RGB2BGR)
        
    except Exception as e:
        logger.error(f"âŒ æ–‡å­—ç¹ªè£½å¤±æ•—: {e}")
        return image_array

def calculate_smart_label_positions(
    detections: Dict[str, np.ndarray], 
    image_size: tuple, 
    font_size: int
) -> List[tuple]:
    """æ™ºèƒ½è¨ˆç®—æ¨™ç±¤ä½ç½®ï¼Œé¿å…é‡ç–Šå’Œé®æ“‹æª¢æ¸¬æ¡†"""
    w, h = image_size
    boxes = detections['xyxy']
    confidences = detections['confidence']
    
    if len(boxes) == 0:
        return []
    
    label_height = font_size + 8
    max_label_width = font_size * 12
    
    label_positions = []
    occupied_regions = []
    
    # æŒ‰ä¿¡å¿ƒåº¦æ’åº
    sorted_indices = np.argsort(confidences)[::-1]
    
    for idx in sorted_indices:
        box = boxes[idx]
        class_id = detections['class_id'][idx]
        conf = confidences[idx]
        
        pill_name = class_names[class_id] if class_id < len(class_names) else f"Unknown_{class_id}"
        label_text = f"{pill_name} {conf:.2f}"
        label_width = min(len(label_text) * font_size * 0.6, max_label_width)
        
        x1, y1, x2, y2 = box.astype(int)
        
        # å€™é¸ä½ç½® - å¢åŠ æ›´å¤šé¸é …
        candidates = [
            # ä¸Šæ–¹ - å¤šå€‹ä½ç½®
            (x1, y1 - label_height - 5, x1 + label_width, y1 - 5),
            (x1 + (x2-x1)//4, y1 - label_height - 5, x1 + (x2-x1)//4 + label_width, y1 - 5),
            (max(0, x2 - label_width), y1 - label_height - 5, x2, y1 - 5),
            
            # ä¸‹æ–¹ - å¤šå€‹ä½ç½®  
            (x1, y2 + 5, x1 + label_width, y2 + 5 + label_height),
            (x1 + (x2-x1)//4, y2 + 5, x1 + (x2-x1)//4 + label_width, y2 + 5 + label_height),
            (max(0, x2 - label_width), y2 + 5, x2, y2 + 5 + label_height),
            
            # å·¦å´
            (x1 - label_width - 5, y1, x1 - 5, y1 + label_height),
            (x1 - label_width - 5, y1 + (y2-y1)//4, x1 - 5, y1 + (y2-y1)//4 + label_height),
            
            # å³å´
            (x2 + 5, y1, x2 + 5 + label_width, y1 + label_height),
            (x2 + 5, y1 + (y2-y1)//4, x2 + 5 + label_width, y1 + (y2-y1)//4 + label_height),
            
            # æ¡†å…§ä¸Šæ–¹
            (x1 + 5, y1 + 5, x1 + 5 + label_width, y1 + 5 + label_height),
            # æ¡†å…§ä¸‹æ–¹
            (x1 + 5, y2 - label_height - 5, x1 + 5 + label_width, y2 - 5),
        ]
        
        # æ‰¾æœ€ä½³ä½ç½®
        best_position = None
        for pos_x1, pos_y1, pos_x2, pos_y2 in candidates:
            # 1. é‚Šç•Œæª¢æŸ¥
            if pos_x1 < 0 or pos_y1 < 0 or pos_x2 > w or pos_y2 > h:
                continue
            
            # 2. èˆ‡å…¶ä»–æ¨™ç±¤é‡ç–Šæª¢æŸ¥
            overlaps_label = False
            for occupied in occupied_regions:
                if not (pos_x2 < occupied[0] or pos_x1 > occupied[2] or 
                       pos_y2 < occupied[1] or pos_y1 > occupied[3]):
                    overlaps_label = True
                    break
            
            if overlaps_label:
                continue
            
            # 3. ğŸ” æª¢æŸ¥æ˜¯å¦é®æ“‹å…¶ä»–æª¢æ¸¬æ¡† (é€™æ˜¯é—œéµï¼)
            blocks_other_boxes = False
            for other_idx in range(len(boxes)):
                if other_idx == idx:  # è·³éè‡ªå·±
                    continue
                    
                other_box = boxes[other_idx]
                ox1, oy1, ox2, oy2 = other_box.astype(int)
                
                # è¨ˆç®—é‡ç–Šé¢ç©
                overlap_x1 = max(pos_x1, ox1)
                overlap_y1 = max(pos_y1, oy1)
                overlap_x2 = min(pos_x2, ox2)
                overlap_y2 = min(pos_y2, oy2)
                
                if overlap_x1 < overlap_x2 and overlap_y1 < overlap_y2:
                    overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)
                    box_area = (ox2 - ox1) * (oy2 - oy1)
                    
                    # ğŸ¯ å¦‚æœæ¨™ç±¤é®æ“‹å…¶ä»–æ¡†è¶…é 20%ï¼Œå‰‡ä¸åˆé©
                    if overlap_area > box_area * 0.2:  # å¾ 30% é™åˆ° 20% æ›´åš´æ ¼
                        blocks_other_boxes = True
                        break
            
            if not blocks_other_boxes:
                best_position = (pos_x1, pos_y1, pos_x2, pos_y2)
                break
        
        # 4. ğŸš¨ å‚™æ´ä½ç½® - å¦‚æœéƒ½ä¸è¡Œï¼Œå°‹æ‰¾åœ–åƒé‚Šç·£ç©ºç™½å€åŸŸ
        if best_position is None:
            # å˜—è©¦åœ–åƒå››å€‹è§’è½
            corner_candidates = [
                # å·¦ä¸Šè§’
                (5, 5, 5 + label_width, 5 + label_height),
                # å³ä¸Šè§’  
                (w - label_width - 5, 5, w - 5, 5 + label_height),
                # å·¦ä¸‹è§’
                (5, h - label_height - 5, 5 + label_width, h - 5),
                # å³ä¸‹è§’
                (w - label_width - 5, h - label_height - 5, w - 5, h - 5),
            ]
            
            for pos_x1, pos_y1, pos_x2, pos_y2 in corner_candidates:
                # æª¢æŸ¥è§’è½ä½ç½®æ˜¯å¦èˆ‡å·²æœ‰æ¨™ç±¤é‡ç–Š
                corner_overlap = False
                for occupied in occupied_regions:
                    if not (pos_x2 < occupied[0] or pos_x1 > occupied[2] or 
                           pos_y2 < occupied[1] or pos_y1 > occupied[3]):
                        corner_overlap = True
                        break
                
                if not corner_overlap:
                    best_position = (pos_x1, pos_y1, pos_x2, pos_y2)
                    break
            
            # 5. ğŸ”§ æœ€çµ‚å‚™æ´ï¼šå¼·åˆ¶æ”¾åœ¨æ¡†ä¸Šæ–¹ï¼ˆå³ä½¿å¯èƒ½é‡ç–Šï¼‰
            if best_position is None:
                pos_x1 = max(5, min(x1, w - label_width - 5))
                pos_y1 = max(label_height + 5, y1 - 10)
                best_position = (pos_x1, pos_y1, pos_x1 + label_width, pos_y1 + label_height)
        
        # è¨˜éŒ„ä½ç½®
        label_positions.append((best_position[0], best_position[1] + label_height))
        occupied_regions.append(best_position)
    
    # é‡æ–°æ’åºåˆ°åŸå§‹é †åº
    final_positions = [None] * len(boxes)
    for i, idx in enumerate(sorted_indices):
        final_positions[idx] = label_positions[i]
    
    return final_positions

def save_image_to_base64(image: Image.Image) -> str:
    """å°‡åœ–ç‰‡è½‰ç‚º base64"""
    buffer = io.BytesIO()
    image.save(buffer, format='JPEG', quality=90)
    import base64
    return base64.b64encode(buffer.getvalue()).decode()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ‡‰ç”¨ç”Ÿå‘½é€±æœŸç®¡ç†"""
    global onnx_session, class_names
    
    try:
        logger.info("ğŸš€ å•Ÿå‹•è—¥ä¸¸æª¢æ¸¬ API...")
        
        # è¼‰å…¥é¡åˆ¥åç¨±
        class_names = load_class_names_from_coco()
        if class_names is None:
            raise RuntimeError("ç„¡æ³•è¼‰å…¥é¡åˆ¥åç¨±")
        
        # è¼‰å…¥ ONNX æ¨¡å‹
        if not os.path.exists(Config.MODEL_PATH):
            raise RuntimeError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {Config.MODEL_PATH}")
        
        logger.info(f"ğŸ“¦ è¼‰å…¥ ONNX æ¨¡å‹: {Config.MODEL_PATH}")
        onnx_session = ort.InferenceSession(
            Config.MODEL_PATH,
            providers=['CPUExecutionProvider']
        )
        
        # æ¨¡å‹ä¿¡æ¯
        input_info = onnx_session.get_inputs()[0]
        output_info = onnx_session.get_outputs()
        
        logger.info(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        logger.info(f"ğŸ“¥ è¼¸å…¥: {input_info.name}{input_info.shape}")
        logger.info(f"ğŸ“¤ è¼¸å‡º: {[out.name + str(out.shape) for out in output_info]}")
        logger.info(f"ğŸ¯ æ”¯æ´ {len(class_names)} ç¨®è—¥ä¸¸")
        logger.info("ğŸŒŸ API æº–å‚™å°±ç·’ï¼")
        
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–å¤±æ•—: {e}")
        raise
    
    yield
    
    logger.info("ğŸ›‘ é—œé–‰è—¥ä¸¸æª¢æ¸¬ API")
    onnx_session = None
    class_names = None

# ğŸš¦ å‰µå»ºé€Ÿç‡é™åˆ¶å™¨
limiter = Limiter(key_func=get_remote_address)

# FastAPI æ‡‰ç”¨
app = FastAPI(
    title="ğŸ’Š Pill Detection API",
    description="AI è—¥ä¸¸è­˜åˆ¥æœå‹™ - ä½¿ç”¨ RF-DETR æ¨¡å‹é€²è¡Œé«˜ç²¾åº¦è—¥ä¸¸æª¢æ¸¬",
    version="2.1.0",
    lifespan=lifespan
)

# ğŸš¦ è¨­ç½®é™æµå™¨å’Œç•°å¸¸è™•ç†
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è«‹æ±‚/å›æ‡‰æ¨¡å‹
class DetectionRequest(BaseModel):
    image_url: HttpUrl = Field(..., description="åœ–åƒ URL")
    threshold: float = Field(
        default=Config.DEFAULT_THRESHOLD, 
        ge=0.1, le=1.0, 
        description="æª¢æ¸¬ä¿¡å¿ƒåº¦é–¾å€¼ (0.1-1.0)"
    )

class DetectionResult(BaseModel):
    detection_id: int
    class_id: int
    pill_name: str
    confidence: float
    bbox: List[float]

class DetectionResponse(BaseModel):
    success: bool
    detections: List[DetectionResult]
    annotated_image_base64: str
    inference_time_ms: float
    total_detections: int

# API ç«¯é»
@app.get("/", tags=["ç³»çµ±"])
@limiter.limit("120/minute")
async def root(request: Request):
    """API æ ¹ç«¯é»"""
    return {
        "name": "ğŸ’Š Pill Detection API",
        "version": "2.1.0",
        "status": "running",
        "model": "RF-DETR ONNX",
        "supported_pills": len(class_names) if class_names else 0
    }

@app.get("/health", tags=["ç³»çµ±"])
@limiter.limit("60/minute")
async def health_check(request: Request):
    """å¥åº·æª¢æŸ¥"""
    return {
        "status": "healthy",
        "model_loaded": onnx_session is not None,
        "classes_loaded": class_names is not None,
        "total_classes": len(class_names) if class_names else 0,
        "memory_usage": "OK",
        "timestamp": time.time()
    }

@app.get("/classes", tags=["æ¨¡å‹"])
@limiter.limit("30/minute")
async def get_supported_classes(request: Request):
    """ç²å–æ‰€æœ‰æ”¯æ´çš„è—¥ä¸¸é¡åˆ¥"""
    if not class_names:
        raise HTTPException(status_code=503, detail="é¡åˆ¥åç¨±æœªè¼‰å…¥")
    
    return {
        "total_classes": len(class_names),
        "classes": [
            {"id": i, "name": name} 
            for i, name in enumerate(class_names)
        ]
    }

@app.post("/detect", response_model=DetectionResponse, tags=["æª¢æ¸¬"])
@limiter.limit("10/minute")  # ğŸš¦ æ¯åˆ†é˜æœ€å¤š 10 æ¬¡è«‹æ±‚
async def detect_pills(request: Request, detection_request: DetectionRequest):
    """
    è—¥ä¸¸æª¢æ¸¬ä¸»ç«¯é»
    
    ä¸Šå‚³åœ–åƒ URLï¼Œè¿”å›æª¢æ¸¬åˆ°çš„è—¥ä¸¸ä¿¡æ¯å’Œæ¨™è¨»åœ–åƒ
    """
    if not onnx_session or not class_names:
        raise HTTPException(status_code=503, detail="æœå‹™æœªå°±ç·’")
    
    try:
        # ğŸ”½ ä¸‹è¼‰åœ–åƒ
        logger.info(f"ğŸ“¥ ä¸‹è¼‰åœ–åƒ: {detection_request.image_url}")
        response = requests.get(str(detection_request.image_url), timeout=15)
        response.raise_for_status()
        
        pil_image = Image.open(io.BytesIO(response.content)).convert('RGB')
        orig_w, orig_h = pil_image.size
        logger.info(f"ğŸ–¼ï¸ åœ–åƒå°ºå¯¸: {orig_w}x{orig_h}")
        
        # ğŸ”„ é è™•ç†
        image_array = np.array(pil_image)
        input_tensor = preprocess_image_for_onnx(image_array)
        
        # ğŸ¤– æ¨ç†
        start_time = time.time()
        input_name = onnx_session.get_inputs()[0].name
        outputs = onnx_session.run(None, {input_name: input_tensor})
        inference_time = (time.time() - start_time) * 1000
        
        # ğŸ” å¾Œè™•ç†
        detections = postprocess_onnx_output(
            outputs, 
            threshold=detection_request.threshold,
            target_size=(orig_w, orig_h)
        )
        
        total_detections = len(detections['class_id'])
        logger.info(f"ğŸ¯ æª¢æ¸¬åˆ° {total_detections} å€‹è—¥ä¸¸")
        
        # ğŸ¨ ç¹ªè£½çµæœ - ä¿®æ­£ç¹ªè£½é †åº
        annotated_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
        
        if total_detections > 0:
            h, w = image_array.shape[:2]
            font_size = max(16, int(min(w, h) / 35))
            thickness = max(2, int(min(w, h) / 400))
            
            # æ™ºèƒ½æ¨™ç±¤ä½ç½®
            label_positions = calculate_smart_label_positions(detections, (w, h), font_size)
            
            # ğŸ¯ æ­¥é©Ÿ1: å…ˆç¹ªè£½æ‰€æœ‰æª¢æ¸¬æ¡†
            for i, (box, conf, class_id) in enumerate(zip(
                detections['xyxy'], 
                detections['confidence'], 
                detections['class_id']
            )):
                x1, y1, x2, y2 = box.astype(int)
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(w, x2), min(h, y2)
                
                color = Config.COLORS[class_id % len(Config.COLORS)]
                
                # åªç¹ªè£½é‚Šç•Œæ¡†ï¼Œä¸ç¹ªè£½æ¨™ç±¤
                cv2.rectangle(annotated_array, (x1, y1), (x2, y2), color, thickness)
            
            # ğŸ·ï¸ æ­¥é©Ÿ2: å†ç¹ªè£½æ‰€æœ‰æ¨™ç±¤ (é€™æ¨£æ¨™ç±¤æœƒåœ¨æ¡†çš„ä¸Šå±¤)
            for i, (box, conf, class_id) in enumerate(zip(
                detections['xyxy'], 
                detections['confidence'], 
                detections['class_id']
            )):
                pill_name = class_names[class_id] if class_id < len(class_names) else f"Unknown_{class_id}"
                color = Config.COLORS[class_id % len(Config.COLORS)]
                
                # ç¹ªè£½æ¨™ç±¤
                label = f"{pill_name} {conf:.2f}"
                text_x, text_y = label_positions[i]
                annotated_array = draw_high_quality_text(
                    annotated_array, label, (text_x, text_y), font_size, color
                )
        
        # ğŸ“¤ è¼¸å‡ºçµæœ
        annotated_pil = Image.fromarray(cv2.cvtColor(annotated_array, cv2.COLOR_BGR2RGB))
        image_base64 = save_image_to_base64(annotated_pil)
        
        detection_results = [
            DetectionResult(
                detection_id=i + 1,
                class_id=int(class_id),
                pill_name=class_names[class_id] if class_id < len(class_names) else f"Unknown_{class_id}",
                confidence=float(conf),
                bbox=[float(x) for x in box]
            )
            for i, (class_id, conf, box) in enumerate(zip(
                detections['class_id'], 
                detections['confidence'], 
                detections['xyxy']
            ))
        ]
        
        logger.info(f"âœ… è™•ç†å®Œæˆ - æª¢æ¸¬:{total_detections}, æ¨ç†:{inference_time:.1f}ms")
        
        return DetectionResponse(
            success=True,
            detections=detection_results,
            annotated_image_base64=image_base64,
            inference_time_ms=round(inference_time, 2),
            total_detections=total_detections
        )
        
    except requests.RequestException as e:
        logger.error(f"âŒ åœ–åƒä¸‹è¼‰å¤±æ•—: {e}")
        raise HTTPException(status_code=400, detail="ç„¡æ³•ä¸‹è¼‰åœ–åƒ")
    except Exception as e:
        logger.error(f"âŒ æª¢æ¸¬å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"æª¢æ¸¬è™•ç†å¤±æ•—: {str(e)}")

@app.post("/detect-file", tags=["æª¢æ¸¬"])
@limiter.limit("10/minute")
async def detect_pills_from_file(
    request: Request,
    file: UploadFile = File(..., description="ä¸Šå‚³çš„åœ–ç‰‡æª”æ¡ˆ"),
    threshold: float = Form(default=Config.DEFAULT_THRESHOLD, ge=0.1, le=1.0)
):
    """
    ç›´æ¥ä¸Šå‚³åœ–ç‰‡æª”æ¡ˆé€²è¡Œè—¥ä¸¸æª¢æ¸¬
    
    æ”¯æ´ JPG, PNG ç­‰å¸¸è¦‹åœ–ç‰‡æ ¼å¼
    """
    if not onnx_session or not class_names:
        raise HTTPException(status_code=503, detail="æœå‹™æœªå°±ç·’")
    
    try:
        # æª¢æŸ¥æª”æ¡ˆé¡å‹
        if not file.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="è«‹ä¸Šå‚³åœ–ç‰‡æª”æ¡ˆ")
        
        # ğŸ“¥ è®€å–ä¸Šå‚³çš„åœ–ç‰‡
        logger.info(f"ğŸ“¥ è™•ç†ä¸Šå‚³æª”æ¡ˆ: {file.filename}")
        image_data = await file.read()
        pil_image = Image.open(io.BytesIO(image_data)).convert('RGB')
        orig_w, orig_h = pil_image.size
        logger.info(f"ğŸ–¼ï¸ åœ–åƒå°ºå¯¸: {orig_w}x{orig_h}")
        
        # ğŸ”„ é è™•ç†
        image_array = np.array(pil_image)
        input_tensor = preprocess_image_for_onnx(image_array)
        
        # ğŸ¤– æ¨ç†
        start_time = time.time()
        input_name = onnx_session.get_inputs()[0].name
        outputs = onnx_session.run(None, {input_name: input_tensor})
        inference_time = (time.time() - start_time) * 1000
        
        # ğŸ” å¾Œè™•ç†
        detections = postprocess_onnx_output(
            outputs, 
            threshold=threshold,
            target_size=(orig_w, orig_h)
        )
        
        total_detections = len(detections['class_id'])
        logger.info(f"ğŸ¯ æª¢æ¸¬åˆ° {total_detections} å€‹è—¥ä¸¸")
        
        # ğŸ¨ ç¹ªè£½çµæœ (ä½¿ç”¨ç›¸åŒçš„ç¹ªè£½é‚è¼¯)
        annotated_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
        
        if total_detections > 0:
            h, w = image_array.shape[:2]
            font_size = max(16, int(min(w, h) / 35))
            thickness = max(2, int(min(w, h) / 400))
            
            # æ™ºèƒ½æ¨™ç±¤ä½ç½®
            label_positions = calculate_smart_label_positions(detections, (w, h), font_size)
            
            # å…ˆç¹ªè£½æ‰€æœ‰æª¢æ¸¬æ¡†
            for i, (box, conf, class_id) in enumerate(zip(
                detections['xyxy'], 
                detections['confidence'], 
                detections['class_id']
            )):
                x1, y1, x2, y2 = box.astype(int)
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(w, x2), min(h, y2)
                
                color = Config.COLORS[class_id % len(Config.COLORS)]
                cv2.rectangle(annotated_array, (x1, y1), (x2, y2), color, thickness)
            
            # å†ç¹ªè£½æ‰€æœ‰æ¨™ç±¤
            for i, (box, conf, class_id) in enumerate(zip(
                detections['xyxy'], 
                detections['confidence'], 
                detections['class_id']
            )):
                pill_name = class_names[class_id] if class_id < len(class_names) else f"Unknown_{class_id}"
                color = Config.COLORS[class_id % len(Config.COLORS)]
                
                label = f"{pill_name} {conf:.2f}"
                text_x, text_y = label_positions[i]
                annotated_array = draw_high_quality_text(
                    annotated_array, label, (text_x, text_y), font_size, color
                )
        
        # ğŸ“¤ è¼¸å‡ºçµæœ
        annotated_pil = Image.fromarray(cv2.cvtColor(annotated_array, cv2.COLOR_BGR2RGB))
        image_base64 = save_image_to_base64(annotated_pil)
        
        detection_results = [
            DetectionResult(
                detection_id=i + 1,
                class_id=int(class_id),
                pill_name=class_names[class_id] if class_id < len(class_names) else f"Unknown_{class_id}",
                confidence=float(conf),
                bbox=[float(x) for x in box]
            )
            for i, (class_id, conf, box) in enumerate(zip(
                detections['class_id'], 
                detections['confidence'], 
                detections['xyxy']
            ))
        ]
        
        logger.info(f"âœ… æª”æ¡ˆè™•ç†å®Œæˆ - æª¢æ¸¬:{total_detections}, æ¨ç†:{inference_time:.1f}ms")
        
        return {
            "success": True,
            "filename": file.filename,
            "detections": detection_results,
            "annotated_image_base64": image_base64,
            "inference_time_ms": round(inference_time, 2),
            "total_detections": total_detections
        }
        
    except Exception as e:
        logger.error(f"âŒ æª”æ¡ˆæª¢æ¸¬å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"æª”æ¡ˆè™•ç†å¤±æ•—: {str(e)}")

@app.get("/test", response_class=HTMLResponse, tags=["å·¥å…·"])
async def test_page():
    """Web æ¸¬è©¦ç•Œé¢ - ç›´æ¥ä¸Šå‚³åœ–ç‰‡é€²è¡Œæª¢æ¸¬"""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>ğŸ’Š è—¥ä¸¸åµæ¸¬æ¸¬è©¦</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <style>
            body { 
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; 
                max-width: 900px; 
                margin: 0 auto; 
                padding: 20px; 
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                min-height: 100vh;
            }
            .container {
                background: white;
                padding: 30px;
                border-radius: 15px;
                box-shadow: 0 10px 30px rgba(0,0,0,0.2);
            }
            h1 { 
                color: #4a5568; 
                text-align: center; 
                margin-bottom: 10px;
                font-size: 2.5em;
            }
            .subtitle {
                text-align: center;
                color: #718096;
                margin-bottom: 30px;
                font-size: 1.1em;
            }
            .form-group { 
                margin: 20px 0; 
            }
            label { 
                display: block; 
                margin-bottom: 8px; 
                font-weight: 600;
                color: #2d3748;
            }
            input[type="file"] { 
                width: 100%; 
                padding: 12px; 
                border: 2px dashed #cbd5e0;
                border-radius: 8px;
                background: #f7fafc;
                transition: all 0.3s;
            }
            input[type="file"]:hover {
                border-color: #667eea;
                background: #edf2f7;
            }
            input[type="number"] { 
                width: 200px; 
                padding: 10px; 
                border: 2px solid #e2e8f0;
                border-radius: 6px;
                font-size: 16px;
            }
            button { 
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white; 
                padding: 12px 30px; 
                border: none; 
                border-radius: 25px; 
                cursor: pointer;
                font-size: 16px;
                font-weight: 600;
                transition: transform 0.2s;
                box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
            }
            button:hover { 
                transform: translateY(-2px);
                box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
            }
            button:disabled {
                background: #a0aec0;
                cursor: not-allowed;
                transform: none;
            }
            #results { 
                margin-top: 30px; 
                padding: 20px; 
                border: 1px solid #e2e8f0; 
                border-radius: 10px;
                background: #f8fafc;
            }
            .detection { 
                background: linear-gradient(135deg, #e6fffa 0%, #b2f5ea 100%);
                padding: 15px; 
                margin: 10px 0; 
                border-radius: 8px;
                border-left: 4px solid #38b2ac;
            }
            .detection strong {
                color: #2c7a7b;
                font-size: 1.1em;
            }
            img { 
                max-width: 100%; 
                height: auto; 
                margin-top: 15px; 
                border-radius: 10px;
                box-shadow: 0 4px 15px rgba(0,0,0,0.1);
            }
            .loading {
                display: flex;
                align-items: center;
                justify-content: center;
                padding: 20px;
            }
            .spinner {
                border: 3px solid #f3f3f3;
                border-top: 3px solid #667eea;
                border-radius: 50%;
                width: 30px;
                height: 30px;
                animation: spin 1s linear infinite;
                margin-right: 10px;
            }
            @keyframes spin {
                0% { transform: rotate(0deg); }
                100% { transform: rotate(360deg); }
            }
            .error {
                background: linear-gradient(135deg, #fed7d7 0%, #feb2b2 100%);
                border-left: 4px solid #e53e3e;
                color: #742a2a;
            }
            .success-stats {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                gap: 15px;
                margin: 20px 0;
            }
            .stat-card {
                background: white;
                padding: 15px;
                border-radius: 8px;
                text-align: center;
                box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            }
            .stat-number {
                font-size: 1.8em;
                font-weight: bold;
                color: #667eea;
            }
            .stat-label {
                color: #718096;
                font-size: 0.9em;
                margin-top: 5px;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸ” è—¥ä¸¸åµæ¸¬ç³»çµ±</h1>
            <p class="subtitle">ä¸Šå‚³åœ–ç‰‡ï¼ŒAI ç«‹å³è­˜åˆ¥è—¥ä¸¸ç¨®é¡</p>
            
            <form id="uploadForm" enctype="multipart/form-data">
                <div class="form-group">
                    <label>ğŸ“· é¸æ“‡åœ–ç‰‡æª”æ¡ˆ:</label>
                    <input type="file" id="imageFile" accept="image/*" required>
                    <small style="color: #718096;">æ”¯æ´ JPG, PNG, JPEG ç­‰æ ¼å¼</small>
                </div>
                
                <div class="form-group">
                    <label>ğŸ¯ ä¿¡å¿ƒåº¦é–¾å€¼:</label>
                    <input type="number" id="threshold" value="0.5" min="0.1" max="1.0" step="0.1">
                    <small style="color: #718096;">æ•¸å€¼è¶Šé«˜ï¼Œæª¢æ¸¬è¶Šåš´æ ¼ (å»ºè­° 0.3-0.7)</small>
                </div>
                
                <div class="form-group">
                    <button type="submit" id="submitBtn">ğŸš€ é–‹å§‹åµæ¸¬</button>
                </div>
            </form>
            
            <div id="results" style="display:none;"></div>
        </div>
        
        <script>
        document.getElementById('uploadForm').onsubmit = async function(e) {
            e.preventDefault();
            
            const fileInput = document.getElementById('imageFile');
            const thresholdInput = document.getElementById('threshold');
            const submitBtn = document.getElementById('submitBtn');
            
            if (!fileInput.files[0]) {
                alert('è«‹å…ˆé¸æ“‡åœ–ç‰‡æª”æ¡ˆï¼');
                return;
            }
            
            // æª¢æŸ¥æª”æ¡ˆå¤§å° (é™åˆ¶ 10MB)
            if (fileInput.files[0].size > 10 * 1024 * 1024) {
                alert('æª”æ¡ˆå¤ªå¤§ï¼è«‹é¸æ“‡å°æ–¼ 10MB çš„åœ–ç‰‡ã€‚');
                return;
            }
            
            const formData = new FormData();
            formData.append('file', fileInput.files[0]);
            formData.append('threshold', thresholdInput.value);
            
            const resultsDiv = document.getElementById('results');
            resultsDiv.style.display = 'block';
            
            // é¡¯ç¤ºè¼‰å…¥ç‹€æ…‹
            submitBtn.disabled = true;
            submitBtn.innerHTML = 'ğŸ”„ åµæ¸¬ä¸­...';
            resultsDiv.innerHTML = `
                <div class="loading">
                    <div class="spinner"></div>
                    <span>AI æ­£åœ¨åˆ†æåœ–ç‰‡ï¼Œè«‹ç¨å€™...</span>
                </div>
            `;
            
            try {
                const response = await fetch('/detect-file', {
                    method: 'POST',
                    body: formData
                });
                
                if (!response.ok) {
                    const errorData = await response.json();
                    throw new Error(errorData.detail || `HTTP ${response.status}`);
                }
                
                const result = await response.json();
                
                let html = '<h2>ğŸ“Š åµæ¸¬çµæœ</h2>';
                
                // çµ±è¨ˆå¡ç‰‡
                html += '<div class="success-stats">';
                html += `
                    <div class="stat-card">
                        <div class="stat-number">${result.total_detections}</div>
                        <div class="stat-label">åµæ¸¬åˆ°çš„è—¥ä¸¸</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">${result.inference_time_ms}ms</div>
                        <div class="stat-label">æ¨è«–æ™‚é–“</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">${(parseFloat(thresholdInput.value) * 100).toFixed(0)}%</div>
                        <div class="stat-label">ä¿¡å¿ƒåº¦é–¾å€¼</div>
                    </div>
                `;
                html += '</div>';
                
                html += `<p><strong>æª”æ¡ˆåç¨±:</strong> ${result.filename}</p>`;
                
                if (result.total_detections > 0) {
                    html += '<h3>ğŸ” åµæ¸¬è©³æƒ…:</h3>';
                    result.detections.forEach((detection, index) => {
                        html += `<div class="detection">
                            <strong>${index + 1}. ${detection.pill_name}</strong><br>
                            <span style="font-size: 0.9em;">
                                ğŸ¯ ä¿¡å¿ƒåº¦: ${(detection.confidence * 100).toFixed(1)}% | 
                                ğŸ“ ä½ç½®: (${Math.round(detection.bbox[0])}, ${Math.round(detection.bbox[1])}) - 
                                (${Math.round(detection.bbox[2])}, ${Math.round(detection.bbox[3])})
                            </span>
                        </div>`;
                    });
                } else {
                    html += '<div class="detection" style="background: #fef5e7; border-left-color: #ed8936;">';
                    html += '<strong>ğŸ¤·â€â™‚ï¸ æœªåµæ¸¬åˆ°è—¥ä¸¸</strong><br>';
                    html += '<span style="font-size: 0.9em;">å¯ä»¥å˜—è©¦é™ä½ä¿¡å¿ƒåº¦é–¾å€¼ï¼Œæˆ–ç¢ºèªåœ–ç‰‡ä¸­æ˜¯å¦åŒ…å«æ”¯æ´çš„è—¥ä¸¸é¡å‹ã€‚</span>';
                    html += '</div>';
                }
                
                if (result.annotated_image_base64) {
                    html += '<h3>ğŸ“¸ æ¨™è¨»çµæœ:</h3>';
                    html += `<img src="data:image/jpeg;base64,${result.annotated_image_base64}" alt="æ¨™è¨»çµæœ">`;
                }
                
                resultsDiv.innerHTML = html;
                
            } catch (error) {
                resultsDiv.innerHTML = `<div class="detection error">
                    <h3>âŒ åµæ¸¬å¤±æ•—</h3>
                    <p><strong>éŒ¯èª¤è¨Šæ¯:</strong> ${error.message}</p>
                    <p>è«‹æª¢æŸ¥ï¼š</p>
                    <ul>
                        <li>åœ–ç‰‡æ ¼å¼æ˜¯å¦æ­£ç¢º (JPG, PNG)</li>
                        <li>æª”æ¡ˆå¤§å°æ˜¯å¦å°æ–¼ 10MB</li>
                        <li>ç¶²è·¯é€£ç·šæ˜¯å¦æ­£å¸¸</li>
                    </ul>
                    <p>å¦‚å•é¡ŒæŒçºŒï¼Œè«‹ç¨å¾Œå†è©¦ã€‚</p>
                </div>`;
            } finally {
                // æ¢å¾©æŒ‰éˆ•ç‹€æ…‹
                submitBtn.disabled = false;
                submitBtn.innerHTML = 'ğŸš€ é–‹å§‹åµæ¸¬';
            }
        }
        
        // æª”æ¡ˆé¸æ“‡é è¦½
        document.getElementById('imageFile').onchange = function(e) {
            const file = e.target.files[0];
            if (file) {
                console.log(`é¸æ“‡äº†æª”æ¡ˆ: ${file.name} (${(file.size/1024/1024).toFixed(2)}MB)`);
            }
        }
        </script>
    </body>
    </html>
    """

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app", 
        host="0.0.0.0", 
        port=8000, 
        reload=True,
        log_level="info"
    )